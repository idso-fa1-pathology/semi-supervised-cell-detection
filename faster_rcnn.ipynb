{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Faster RCNN\n",
    "\n",
    "Code is for the following video: https://www.youtube.com/watch?v=Uc90rr5jbA4&t=71s\n",
    "\n",
    "Do give this notebook a thumbs-up if you liked it. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require the latest version of torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import functional as FT\n",
    "from torchvision import transforms as T\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
    "import copy\n",
    "import math\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A  # our data augmentation library\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove arnings (optional)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # progress bar\n",
    "from torchvision.utils import draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyCOCOTools provides many utilities for dealing with datasets in the COCO format, and if you wanted, you could evaluate the model's performance on the dataset with some of the utilities provided with this library.\n",
    "\n",
    "That is out of scope for this notebook, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our dataset is in cocoformat, we will need pypcoco tools\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will define our transforms\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use albumentations as our data augmentation library due to its capability to deal with bounding boxes in multiple formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train=False):\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(640, 640), # our input size can be 600px\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomBrightnessContrast(p=0.1),\n",
    "            A.ColorJitter(p=0.1),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(640, 640), # our input size can be 600px\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This is our dataset class. It loads all the necessary files and it processes the data so that it can be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "class AquariumDetection(datasets.VisionDataset):\n",
    "    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.split = split  # train, valid, test\n",
    "        self.coco = COCO(os.path.join(root, split, \"_annotations.coco.json\"))  # annotations stored here\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
    "\n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        image = cv2.imread(os.path.join(self.root, self.split, path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "\n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "    def get_image_name(self, index):\n",
    "        \"\"\"Retrieve the image name for the given index.\"\"\"\n",
    "        id = self.ids[index]\n",
    "        return self.coco.loadImgs(id)[0]['file_name']\n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "        target = copy.deepcopy(target)\n",
    "        \n",
    "#         print(f\"Original bounding boxes for {index}: {target}\")\n",
    "    \n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target]  # required annotation format for albumentations\n",
    "    \n",
    "        height, width, _ = image.shape  # Get image dimensions\n",
    "    \n",
    "        # Filter out bounding boxes with invalid coordinates\n",
    "        valid_boxes = []\n",
    "        for box in boxes:\n",
    "            x_min, y_min, w, h, category_id = box\n",
    "            x_max, y_max = x_min + w, y_min + h\n",
    "    \n",
    "            # Adjust the coordinates check to consider the image dimensions\n",
    "            if 0 <= x_min <= width and 0 <= y_min <= height and 0 <= x_max <= width and 0 <= y_max <= height:\n",
    "                valid_boxes.append(box)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=valid_boxes)\n",
    "            image = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "\n",
    "        new_boxes = []  # convert from xywh to xyxy\n",
    "        for box in boxes:\n",
    "            xmin = box[0]\n",
    "            xmax = xmin + box[2]\n",
    "            ymin = box[1]\n",
    "            ymax = ymin + box[3]\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Initialize 'targ' dictionary with keys and empty tensors\n",
    "        targ = {\n",
    "            'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
    "            'labels': torch.tensor([], dtype=torch.int64),\n",
    "            'image_id': torch.tensor([], dtype=torch.int64),\n",
    "            'area': torch.tensor([], dtype=torch.float32),\n",
    "            'iscrowd': torch.tensor([], dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        # Update 'targ' dictionary if there are valid boxes\n",
    "        if new_boxes:\n",
    "            boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "            targ['boxes'] = boxes\n",
    "            targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
    "            targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
    "            targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])  # different area calculation\n",
    "            targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "            \n",
    "#         print(f\"Processed bounding boxes for {index}: {boxes}\")\n",
    "\n",
    "        return image.div(255), targ  # scale images\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/rsrch5/home/plm/yshokrollahi/dataset_frcnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load classes\n",
    "coco = COCO(os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"))\n",
    "categories = coco.cats\n",
    "n_classes = len(categories.keys())\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code just gets a list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [i[1]['name'] for i in categories.items()]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AquariumDetection(root=dataset_path, transforms=get_transforms(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sample image and its bounding boxes, this code does not get the model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample\n",
    "sample = train_dataset[23]\n",
    "img_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\n",
    "\n",
    "# Retrieve and print the image name\n",
    "image_name = train_dataset.get_image_name(50)\n",
    "print(f\"Image Name: {image_name}\")\n",
    "\n",
    "# Check if there are bounding boxes\n",
    "if len(sample[1]['boxes']) > 0:\n",
    "    print(\"Bounding boxes are present.\")\n",
    "else:\n",
    "    print(\"No bounding boxes.\")\n",
    "\n",
    "# Draw and display the image with bounding boxes (if any)\n",
    "plt.imshow(draw_bounding_boxes(\n",
    "    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4\n",
    ").permute(1, 2, 0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Our model is FasterRCNN with a backbone of `MobileNetV3-Large`. We need to change the output layers because we have just 7 classes but this model was trained on 90 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the faster rcnn model\n",
    "model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features # we need to change the head\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our collating function for the train dataloader, it allows us to create batches of data that can be easily pass into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new batch size\n",
    "new_batch_size = 16  # for example, increase to 8; adjust this based on your GPU memory\n",
    "\n",
    "# Create the DataLoader with the new batch size\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=new_batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=4, \n",
    "                          pin_memory=True,  # set to True if using a GPU\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "# Now you can use the train_loader as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blocks ensures that the model can take in the data and that it will not crash during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,targets = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(image for image in images)\n",
    "targets = [{k:v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets) # just make sure this runs without error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") # use GPU to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Here, we define the optimizer. If you wish, you can also define the LR Scheduler, but it is not necessary for this notebook since our dataset is so small.\n",
    "\n",
    "> Note, there are a few bugs with the current way `lr_scheduler` is implemented. If you wish to use the scheduler, you will have to fix those bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T21:16:51.573991Z",
     "iopub.status.busy": "2022-01-19T21:16:51.573731Z",
     "iopub.status.idle": "2022-01-19T21:16:51.579808Z",
     "shell.execute_reply": "2022-01-19T21:16:51.578956Z",
     "shell.execute_reply.started": "2022-01-19T21:16:51.573954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, and optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[16, 22], gamma=0.1) # lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T21:16:51.904692Z",
     "iopub.status.busy": "2022-01-19T21:16:51.904491Z",
     "iopub.status.idle": "2022-01-19T21:16:51.909408Z",
     "shell.execute_reply": "2022-01-19T21:16:51.907132Z",
     "shell.execute_reply.started": "2022-01-19T21:16:51.90467Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The following is a function that will train the model for one epoch. Torchvision Object Detections models have a loss function built in, and it will calculate the loss automatically if you pass in the `inputs` and `targets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T21:17:07.899073Z",
     "iopub.status.busy": "2022-01-19T21:17:07.898807Z",
     "iopub.status.idle": "2022-01-19T21:17:07.910123Z",
     "shell.execute_reply": "2022-01-19T21:17:07.909449Z",
     "shell.execute_reply.started": "2022-01-19T21:17:07.899043Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "#     lr_scheduler = None\n",
    "#     if epoch == 0:\n",
    "#         warmup_factor = 1.0 / 1000 # do lr warmup\n",
    "#         warmup_iters = min(1000, len(loader) - 1)\n",
    "        \n",
    "#         lr_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor = warmup_factor, total_iters=warmup_iters)\n",
    "    \n",
    "    all_losses = []\n",
    "    all_losses_dict = []\n",
    "    \n",
    "    for images, targets in tqdm(loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets) # the model computes the loss automatically if we pass in targets\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
    "        loss_value = losses.item()\n",
    "        \n",
    "        all_losses.append(loss_value)\n",
    "        all_losses_dict.append(loss_dict_append)\n",
    "        \n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping trainig\") # train if loss becomes infinity\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if lr_scheduler is not None:\n",
    "#             lr_scheduler.step() # \n",
    "        \n",
    "    all_losses_dict = pd.DataFrame(all_losses_dict) # for printing\n",
    "    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
    "        all_losses_dict['loss_classifier'].mean(),\n",
    "        all_losses_dict['loss_box_reg'].mean(),\n",
    "        all_losses_dict['loss_rpn_box_reg'].mean(),\n",
    "        all_losses_dict['loss_objectness'].mean()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Epochs should be enough to train this model for a high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T21:17:09.340845Z",
     "iopub.status.busy": "2022-01-19T21:17:09.340265Z",
     "iopub.status.idle": "2022-01-19T21:20:07.966214Z",
     "shell.execute_reply": "2022-01-19T21:20:07.965482Z",
     "shell.execute_reply.started": "2022-01-19T21:17:09.340804Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 250\n",
    "model_save_path = 'frcnn_models'  # Define the directory to save the models\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    # lr_scheduler.step() # Uncomment if you are using a learning rate scheduler\n",
    "\n",
    "    # Save the model every 25 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }\n",
    "        save_path = os.path.join(model_save_path, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "        torch.save(checkpoint, save_path)\n",
    "        print(f'Model saved at epoch {epoch + 1} in {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our learning rate was too low, due to a lr scheduler bug. For this task, we wont need a scheudul.er\n",
    "# Save the final trained model\n",
    "torch.save(model.state_dict(), \"frcnn_models/checkpoint_epoch_250.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model (assuming the model architecture is already defined)\n",
    "model.load_state_dict(torch.load(\"frcnn_models/checkpoint_epoch_250.pth\"))\n",
    "model.eval()  # Set the model to evaluation mode if you are doing inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying on sample Images\n",
    "\n",
    "This is the inference code for the model. First, we set the model to evaluation mode and clear the GPU Cache. We also load a test dataset, so that we can use fresh images that the model hasn't seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T21:20:38.284239Z",
     "iopub.status.busy": "2022-01-19T21:20:38.283963Z",
     "iopub.status.idle": "2022-01-19T21:20:38.315924Z",
     "shell.execute_reply": "2022-01-19T21:20:38.313831Z",
     "shell.execute_reply.started": "2022-01-19T21:20:38.284212Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will watch first epoich to ensure no errrors\n",
    "# while it is training, lets write code to see the models predictions. lets try again\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T21:20:39.546292Z",
     "iopub.status.busy": "2022-01-19T21:20:39.546026Z",
     "iopub.status.idle": "2022-01-19T21:20:39.558164Z",
     "shell.execute_reply": "2022-01-19T21:20:39.557287Z",
     "shell.execute_reply.started": "2022-01-19T21:20:39.546263Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = AquariumDetection(root=dataset_path, split=\"test\", transforms=get_transforms(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T21:21:13.618037Z",
     "iopub.status.busy": "2022-01-19T21:21:13.617425Z",
     "iopub.status.idle": "2022-01-19T21:21:13.680059Z",
     "shell.execute_reply": "2022-01-19T21:21:13.679363Z",
     "shell.execute_reply.started": "2022-01-19T21:21:13.617996Z"
    }
   },
   "outputs": [],
   "source": [
    "img, _ = test_dataset[1]\n",
    "img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])\n",
    "    pred = prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T21:21:13.729977Z",
     "iopub.status.busy": "2022-01-19T21:21:13.729773Z",
     "iopub.status.idle": "2022-01-19T21:21:13.735813Z",
     "shell.execute_reply": "2022-01-19T21:21:13.73509Z",
     "shell.execute_reply.started": "2022-01-19T21:21:13.729953Z"
    }
   },
   "outputs": [],
   "source": [
    "# it did learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T21:21:13.850339Z",
     "iopub.status.busy": "2022-01-19T21:21:13.849773Z",
     "iopub.status.idle": "2022-01-19T21:21:14.30599Z",
     "shell.execute_reply": "2022-01-19T21:21:14.305306Z",
     "shell.execute_reply.started": "2022-01-19T21:21:13.850307Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 10))\n",
    "plt.imshow(draw_bounding_boxes(img_int,\n",
    "    pred['boxes'][pred['scores'] > 0.8],\n",
    "    [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], width=4\n",
    ").permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode and clear the GPU Cache\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load a test dataset image along with its annotations\n",
    "test_dataset = AquariumDetection(root=dataset_path, split=\"test\", transforms=get_transforms(False))\n",
    "img, target = test_dataset[6]\n",
    "img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
    "\n",
    "# Perform prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])\n",
    "    pred = prediction[0]\n",
    "\n",
    "# Draw actual (ground truth) bounding boxes\n",
    "actual_boxes_img = draw_bounding_boxes(\n",
    "    img_int.clone(), \n",
    "    target['boxes'], \n",
    "    [classes[i] for i in target['labels']], \n",
    "    colors='blue', \n",
    "    width=4\n",
    ")\n",
    "\n",
    "# Set confidence threshold\n",
    "confidence_threshold = 0.001\n",
    "\n",
    "# Filter out predictions below the confidence threshold\n",
    "pred_boxes = pred['boxes'][pred['scores'] > confidence_threshold]\n",
    "pred_labels = pred['labels'][pred['scores'] > confidence_threshold]\n",
    "\n",
    "# Draw predicted bounding boxes\n",
    "predicted_boxes_img = draw_bounding_boxes(\n",
    "    img_int.clone(), \n",
    "    pred_boxes, \n",
    "    [classes[i] for i in pred_labels.tolist()], \n",
    "    colors='red', \n",
    "    width=4\n",
    ")\n",
    "\n",
    "# Display the images side by side for comparison\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(actual_boxes_img.permute(1, 2, 0))\n",
    "ax[0].set_title('Real Bounding Boxes')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(predicted_boxes_img.permute(1, 2, 0))\n",
    "ax[1].set_title('Predicted Bounding Boxes')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(model, dataset, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, _ in dataset:\n",
    "            img = img.to(device)\n",
    "            prediction = model([img])\n",
    "            predictions.append(prediction[0])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Assuming test_dataset is already created\n",
    "predictions = get_model_predictions(model, test_dataset, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def prepare_for_coco_evaluation(predictions, dataset):\n",
    "    coco_results = []\n",
    "    for img_id, prediction in zip(dataset.ids, predictions):  # Use actual image IDs from the dataset\n",
    "        boxes = prediction['boxes'].cpu().numpy()\n",
    "        scores = prediction['scores'].cpu().numpy()\n",
    "        labels = prediction['labels'].cpu().numpy()\n",
    "\n",
    "        for idx in range(boxes.shape[0]):\n",
    "            box = boxes[idx].tolist()\n",
    "            score = float(scores[idx])\n",
    "            label = int(labels[idx])\n",
    "\n",
    "            coco_result = {\n",
    "                \"image_id\": img_id,  # Use actual image ID\n",
    "                \"category_id\": label,\n",
    "                \"bbox\": [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                \"score\": score\n",
    "            }\n",
    "            coco_results.append(coco_result)\n",
    "    return coco_results\n",
    "\n",
    "\n",
    "\n",
    "# Convert model predictions to COCO format\n",
    "coco_predictions = prepare_for_coco_evaluation(predictions, test_dataset)\n",
    "\n",
    "# Write to a file (COCO expects a JSON file)\n",
    "with open('predictions.json', 'w') as f:\n",
    "    json.dump(coco_predictions, f)\n",
    "\n",
    "# Load the ground truth annotations\n",
    "cocoGt = COCO('/rsrch5/home/plm/yshokrollahi/dataset_frcnn/test/_annotations.coco.json')\n",
    "\n",
    "# Load the predictions\n",
    "cocoDt = cocoGt.loadRes('predictions.json')\n",
    "\n",
    "# Create COCO Eval object\n",
    "cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "\n",
    "# Evaluate on the dataset\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model architecture exactly as before\n",
    "model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)\n",
    "\n",
    "# Load the weights\n",
    "model.load_state_dict(torch.load(\"frcnn_models/checkpoint_epoch_150.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
    "# If you saved optimizer state, load it here\n",
    "# optimizer.load_state_dict(torch.load(\"optimizer_state.pth\"))\n",
    "\n",
    "num_epochs = 250  # Set the number of epochs you want to train for\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    # Optionally save the model and optimizer state at each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our learning rate was too low, due to a lr scheduler bug. For this task, we wont need a scheudul.er\n",
    "# Save the final trained model\n",
    "torch.save(model.state_dict(), \"frcnn_models/trained_50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode and clear the GPU Cache\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load a test dataset image along with its annotations\n",
    "test_dataset = AquariumDetection(root=dataset_path, split=\"test\", transforms=get_transforms(False))\n",
    "img, target = test_dataset[76]\n",
    "img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
    "\n",
    "# Perform prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])\n",
    "    pred = prediction[0]\n",
    "\n",
    "# Draw actual (ground truth) bounding boxes\n",
    "actual_boxes_img = draw_bounding_boxes(\n",
    "    img_int.clone(), \n",
    "    target['boxes'], \n",
    "    [classes[i] for i in target['labels']], \n",
    "    colors='blue', \n",
    "    width=4\n",
    ")\n",
    "\n",
    "# Set confidence threshold\n",
    "confidence_threshold = 0.001\n",
    "\n",
    "# Filter out predictions below the confidence threshold\n",
    "pred_boxes = pred['boxes'][pred['scores'] > confidence_threshold]\n",
    "pred_labels = pred['labels'][pred['scores'] > confidence_threshold]\n",
    "\n",
    "# Draw predicted bounding boxes\n",
    "predicted_boxes_img = draw_bounding_boxes(\n",
    "    img_int.clone(), \n",
    "    pred_boxes, \n",
    "    [classes[i] for i in pred_labels.tolist()], \n",
    "    colors='red', \n",
    "    width=4\n",
    ")\n",
    "\n",
    "# Display the images side by side for comparison\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(actual_boxes_img.permute(1, 2, 0))\n",
    "ax[0].set_title('Real Bounding Boxes')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(predicted_boxes_img.permute(1, 2, 0))\n",
    "ax[1].set_title('Predicted Bounding Boxes')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1849949,
     "sourceId": 3020523,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
